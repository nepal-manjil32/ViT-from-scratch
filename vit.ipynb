{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Importing Necessary Libraries & Dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.5.1\n",
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "##-- Import libraries --##\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from pytorch_modules import data_setup, engine, utils\n",
    "\n",
    "##-- Setup device agnostic code --##\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PosixPath('chest_xray/train'), PosixPath('chest_xray/val'))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##-- Setup directory paths --##\n",
    "from pathlib import Path\n",
    "image_data_path = Path(\"./chest_xray\")\n",
    "train_dir = image_data_path / \"train\"\n",
    "val_dir = image_data_path / \"val\"\n",
    "\n",
    "train_dir, val_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create Datasets & DataLoaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Creating a transforms for `torchvision.models` (auto creation)\n",
    "\n",
    "- As of `torchvision` v0.13_ there is now support for automatic data transform creation based on the pretrained model weights you're using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViT_B_16_Weights.IMAGENET1K_V1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##-- Get a set of pretrained model weights --##\n",
    "vit_b16_weights =torchvision.models.ViT_B_16_Weights.DEFAULT # 'DEFAULT' = best availabel weight\n",
    "vit_b16_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageClassification(\n",
       "    crop_size=[224]\n",
       "    resize_size=[256]\n",
       "    mean=[0.485, 0.456, 0.406]\n",
       "    std=[0.229, 0.224, 0.225]\n",
       "    interpolation=InterpolationMode.BILINEAR\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##-- Get the transforms used to create the pretrained weights\n",
    "auto_transforms = vit_b16_weights.transforms()\n",
    "auto_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataloader.DataLoader at 0x1192b09b0>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x1294babd0>,\n",
       " ['Covid', 'Normal', 'Viral Pneumonia'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##-- Create DataLoaders using auto_transforms --##\n",
    "train_dataloader, val_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n",
    "                                                                              val_dir=val_dir,\n",
    "                                                                              transform=auto_transforms,\n",
    "                                                                              batch_size=BATCH_SIZE)\n",
    "train_dataloader, val_dataloader, class_names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Getting Pretrained Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Setting Up Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "vit_b16_weights =torchvision.models.ViT_B_16_Weights.DEFAULT # \"DEFAULT\" = get the best available weights\n",
    "model_b16 = torchvision.models.vit_b_16(weights=vit_b16_weights).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"{model_b16.avgpool}\\n\")\n",
    "# print(f\"{model_b16.classifier}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 Getting Model Summary Using `torchinfo`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "============================================================================================================================================\n",
       "Layer (type (var_name))                                      Input Shape          Output Shape         Param #              Trainable\n",
       "============================================================================================================================================\n",
       "VisionTransformer (VisionTransformer)                        [1, 3, 224, 224]     [1, 1000]            768                  True\n",
       "├─Conv2d (conv_proj)                                         [1, 3, 224, 224]     [1, 768, 14, 14]     590,592              True\n",
       "├─Encoder (encoder)                                          [1, 197, 768]        [1, 197, 768]        151,296              True\n",
       "│    └─Dropout (dropout)                                     [1, 197, 768]        [1, 197, 768]        --                   --\n",
       "│    └─Sequential (layers)                                   [1, 197, 768]        [1, 197, 768]        --                   True\n",
       "│    │    └─EncoderBlock (encoder_layer_0)                   [1, 197, 768]        [1, 197, 768]        7,087,872            True\n",
       "│    │    └─EncoderBlock (encoder_layer_1)                   [1, 197, 768]        [1, 197, 768]        7,087,872            True\n",
       "│    │    └─EncoderBlock (encoder_layer_2)                   [1, 197, 768]        [1, 197, 768]        7,087,872            True\n",
       "│    │    └─EncoderBlock (encoder_layer_3)                   [1, 197, 768]        [1, 197, 768]        7,087,872            True\n",
       "│    │    └─EncoderBlock (encoder_layer_4)                   [1, 197, 768]        [1, 197, 768]        7,087,872            True\n",
       "│    │    └─EncoderBlock (encoder_layer_5)                   [1, 197, 768]        [1, 197, 768]        7,087,872            True\n",
       "│    │    └─EncoderBlock (encoder_layer_6)                   [1, 197, 768]        [1, 197, 768]        7,087,872            True\n",
       "│    │    └─EncoderBlock (encoder_layer_7)                   [1, 197, 768]        [1, 197, 768]        7,087,872            True\n",
       "│    │    └─EncoderBlock (encoder_layer_8)                   [1, 197, 768]        [1, 197, 768]        7,087,872            True\n",
       "│    │    └─EncoderBlock (encoder_layer_9)                   [1, 197, 768]        [1, 197, 768]        7,087,872            True\n",
       "│    │    └─EncoderBlock (encoder_layer_10)                  [1, 197, 768]        [1, 197, 768]        7,087,872            True\n",
       "│    │    └─EncoderBlock (encoder_layer_11)                  [1, 197, 768]        [1, 197, 768]        7,087,872            True\n",
       "│    └─LayerNorm (ln)                                        [1, 197, 768]        [1, 197, 768]        1,536                True\n",
       "├─Sequential (heads)                                         [1, 768]             [1, 1000]            --                   True\n",
       "│    └─Linear (head)                                         [1, 768]             [1, 1000]            769,000              True\n",
       "============================================================================================================================================\n",
       "Total params: 86,567,656\n",
       "Trainable params: 86,567,656\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 173.23\n",
       "============================================================================================================================================\n",
       "Input size (MB): 0.60\n",
       "Forward/backward pass size (MB): 104.09\n",
       "Params size (MB): 232.27\n",
       "Estimated Total Size (MB): 336.96\n",
       "============================================================================================================================================"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(model=model_b16,\n",
    "        input_size=(1, 3, 224, 224), # [batch_size, color_channels, height, width],\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5 Freezing Base Model & Changing Output Layer Based On Our Needs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "##-- Freeze all the base layers in EffNetB7 --##\n",
    "for params in model_b16.parameters():\n",
    "    params.requires_grad = False # won't updated the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "##-- Update the classifier head of our model to suite our problem --##\n",
    "from torch import nn\n",
    "\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "torch.cuda.manual_seed(RANDOM_STATE)\n",
    "\n",
    "model_b16.classifier = nn.Sequential(\n",
    "    nn.Dropout(p=0.2, inplace=True),\n",
    "    nn.Linear(in_features=768, # features vector coming in from the forzen layers\n",
    "              out_features=len(class_names)).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "============================================================================================================================================\n",
       "Layer (type (var_name))                                      Input Shape          Output Shape         Param #              Trainable\n",
       "============================================================================================================================================\n",
       "VisionTransformer (VisionTransformer)                        [1, 3, 224, 224]     [1, 1000]            3,075                Partial\n",
       "├─Conv2d (conv_proj)                                         [1, 3, 224, 224]     [1, 768, 14, 14]     (590,592)            False\n",
       "├─Encoder (encoder)                                          [1, 197, 768]        [1, 197, 768]        151,296              False\n",
       "│    └─Dropout (dropout)                                     [1, 197, 768]        [1, 197, 768]        --                   --\n",
       "│    └─Sequential (layers)                                   [1, 197, 768]        [1, 197, 768]        --                   False\n",
       "│    │    └─EncoderBlock (encoder_layer_0)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_1)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_2)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_3)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_4)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_5)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_6)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_7)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_8)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_9)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_10)                  [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_11)                  [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
       "│    └─LayerNorm (ln)                                        [1, 197, 768]        [1, 197, 768]        (1,536)              False\n",
       "├─Sequential (heads)                                         [1, 768]             [1, 1000]            --                   False\n",
       "│    └─Linear (head)                                         [1, 768]             [1, 1000]            (769,000)            False\n",
       "============================================================================================================================================\n",
       "Total params: 86,569,963\n",
       "Trainable params: 2,307\n",
       "Non-trainable params: 86,567,656\n",
       "Total mult-adds (Units.MEGABYTES): 173.23\n",
       "============================================================================================================================================\n",
       "Input size (MB): 0.60\n",
       "Forward/backward pass size (MB): 104.09\n",
       "Params size (MB): 232.27\n",
       "Estimated Total Size (MB): 336.96\n",
       "============================================================================================================================================"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model=model_b16,\n",
    "        input_size=(1, 3, 224, 224), # [batch_size, color_channels, height, width],\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.manual_seed(RANDOM_STATE)\n",
    "# torch.cuda.manual_seed(RANDOM_STATE)\n",
    "\n",
    "# ##-- Define hyperparamters --##\n",
    "# EPOCHS = 20\n",
    "# LEARNING_RATE = 0.001\n",
    "\n",
    "# ##-- Define Loss & Optimizer --##\n",
    "# loss_fn = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.Adam(params=model_b16.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# ##-- Start timer --##\n",
    "# from timeit import default_timer as timer\n",
    "# start_time = timer()\n",
    "\n",
    "# ##-- Setup training and save the results --##\n",
    "# model_b7_results = engine.train(model=model_b16,\n",
    "#                                train_dataloader=train_dataloader,\n",
    "#                                val_dataloader=val_dataloader,\n",
    "#                                optimizer=optimizer,\n",
    "#                                loss_fn=loss_fn,\n",
    "#                                epochs=EPOCHS,\n",
    "#                                device=device)\n",
    "\n",
    "# ##-- End timer --##\n",
    "# end_time = timer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##- Save the model --##\n",
    "# utils.save_model(model=model_b16,\n",
    "#                  target_dir=\"./saved_models\",\n",
    "#                  model_name=f\"efficientnetb7_{LEARNING_RATE}_{EPOCHS}_{BATCH_SIZE}.pth\",\n",
    "#                  optimizer=optimizer,\n",
    "#                  epoch=EPOCHS)\n",
    "\n",
    "# print(f\"[INFO]: Total training time: {(end_time-start_time)/60:.3f} minutes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##-- Plot loss curves --##\n",
    "# epochs = [i for i in range(1, EPOCHS+1)]\n",
    "\n",
    "# plt.figure(figsize=(12, 5))\n",
    "\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.plot(epochs, model_b7_results[\"train_loss\"], color=\"blue\", label=\"train_loss\")\n",
    "# plt.plot(epochs, model_b7_results[\"val_loss\"], color=\"red\", label=\"val_loss\")\n",
    "# plt.xlabel(\"Epochs\")\n",
    "# plt.ylabel(\"Loss\")\n",
    "# plt.legend()\n",
    "\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.plot(epochs, model_b7_results[\"train_acc\"], color=\"blue\", label=\"train_acc\")\n",
    "# plt.plot(epochs, model_b7_results[\"val_acc\"], color=\"green\", label=\"val_acc\")\n",
    "# plt.xlabel(\"Epochs\")\n",
    "# plt.ylabel(\"Accuracy\")\n",
    "# plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
